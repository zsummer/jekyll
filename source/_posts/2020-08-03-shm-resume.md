---
title: 共享内存RESUME综述 
date: 2020-08-03
categories: develop 
author: yawei.zhang 
mathjax: true
---

<!-- toc -->


## 前言    
**共享内存RESUME机制是指的: 通过将游戏状态数据保存在共享内存中, 当游戏进程crash后通过重启游戏服务器并attach已有的共享内存来恢复游戏状态, 以此达到玩家游戏体验在出现宕机时的连贯性, 提升游戏的容灾能力.**       


## 共享内存RESUME出现的情景   

### MMOACT相比传统互联网的异同        

在web领域中,  业务和数据分离, 从而达成'业务无状态化', 开发人员专注于业务本身, 状态数据的一致性问题和容灾问题转移到可以较为独立解决的数据存储领域, 这个领域有非常多的论文和解决方案, 以及成熟的服务等.     

而对于游戏领域来说, 互联网成熟的解决的方案很难在这个地方生效,  当然 对于外围系统来说, 我们仍然可以按照互联网成熟的解决方案进行布局, 例如聊天系统, 好友系统,  邮件系统, 日志系统,  或者一些游戏中的公会系统等.     


MMOACT的特性:  
* **CPU计算密集**   
  * 地图单位扫描选择 命中包围盒碰撞检测 战斗事件响应处理  AI的扫描检测,行为决策,  移动的寻路/碰撞避免/检测等    
  
* **IO密集**  
  * 视野内所有玩家的事件都要同步所有玩家(理想情况下), 这是一个$O(N^2)$的广播复杂度  
  * 大世界内视野范围较为开阔   
  
* **低延迟响应**   
  * 在战斗中, 一次攻击动作被拆解成逐帧开启和关闭的 霸体 无敌  攻击窗口 可闪避窗口 可打断窗口等流程片段, 一帧16.7ms  
  * 在战斗中每秒平均5~7米的移动距离, 100ms的延迟将会带来一个身位的偏差造成命中失败  
  * 弓箭的速度大约每秒50米, 30帧的客户端一帧就有两个身位的偏差  
  * 互联网骨干网20ms的延迟 每一点额外的延迟都会给同步带来较为巨大的压力.   
  
* **共享状态高频读写**   
  * 战斗单位与战斗单位之间, 战斗单位的模块与模块之间   
  * ACT战斗中实时响应判定的AI(大量事件和回调)  
  * 装备, 属性, 能量槽, 状态标签与技能和移动之间的相互引用与保证   
    * BUFF对属性的修改 标签的装载和卸载 子弹时间的进出等都需要严格保证对称   
  * 技能BUFF流程中跨战斗单位跨模块并需要严格保证时序的事件判定与脚本回执等   

* **重业务逻辑,需求易变**   
  * 业务类型繁多复杂且耦合, 见上栏'共享状态高频读写'   


传统互联网特性:  
* 数据规模大 用户量大 并发大  
* 轻业务, 重存储 对数据一致性要求较高    
  * 读写改查这几个基本操作可以涵盖绝大部分互联网业务的核心内容  
* 业务较为稳定  
* **延迟不敏感 通常都是秒级以上**.  
  * google Analytics速度报告中, 网页的平均加载时间为4~8秒   2秒打开网页我们会觉得飞快(秒开)  
  * 互动式直播和视频会议的延迟平均1~3秒  
  * 苹果支付服务器验证一个支付凭据需要3s-6s   
  * 45秒才能看视频   

游戏业务的形式化描述:   
$$
S_k=\begin{cases}
g(P, C), \qquad if \quad k = 0 \\\\
t(S_{k-1}, C, I_k),  \quad if \quad k \geq 1
\end{cases}
$$

> I是游戏状态变化的根本原因的集合 往往是各个玩家(按键)操作  
> S是游戏状态的集合 由众多状态子集组成  

> 该公式的描述: 
> * 游戏在第0个逻辑帧时 根据玩家信息P和游戏配置C 进行初始化运算g 得出初始化状态集合$S_0$    
> * 游戏在第k个逻辑帧时 根据前一个状态集合$S_{k-1}$和游戏配置C  根据第k帧收到的外部变化原因集合$I_k$ 进行逻辑t运算 得出第k个逻辑帧新的游戏状态集合$S_k$   


大部分互联网核心业务都能很好的进行业务和状态存储上的解耦, 以stateless形式在现有成熟的数据库相关的存储服务之上通过已有的成熟的解决方案来订制解决, 其核心往往是通过牺牲响应速度, 提高解决方案的复杂度来实现大规模高一致性的互联网需求.   

(12306的业务也不算复杂 但是难在大规模并发下, 状态之间难以解耦进行传统的分而治之而造成的)

而游戏服务, 很多时候游戏服务可以看成一个庞大的非确定状态机, 有非常庞大的状态集合, 实时响应所有玩家的请求并不断的推演下去,  并且其业务变更非常频繁,  往往是通过牺牲一定程度的可靠性和一致性来做到在有限的开发周期和资源里, 把一个尽可能满足策划设计和玩家体验的游戏做出来.   
实际上大部分的游戏项目也都在解决这个问题, 也因此互联网成熟的解决方案, 流行的解决方案 往往都离游戏项目比较远, 但是意思也并不是无关,  在靠近外围的架构和服务节点上, 我们仍然可以追着互联网潮流进行演进, 例如微服务.    



附图, MMO技能的基本流程如下:   
* 技能释放条件  --> 判断自身脚本 -->判断目标是否有脚本有则等待执行结果   
* 技能预处理   --> 判定  
* 技能释放成功  -->  判定  
* 技能命中开始扫描目标  --> 是否有反向过滤  --> 等待执行结果   
* 技能遍历所有选中目标
  * 即将对目标发起命中处理  --> 判定  
  * 对目标发起命中处理  
    * 遍历所有效果    
      * 即将对目标产生效果   --> 判定 
        * 如果是伤害则有 伤害预处理  -- >判定    
        * 如果是BUFF则有额外的buff流程判定  
      * 已经对目标产生效果  --> 判定  
        * 如果是伤害则有 伤害已经处理  -->判定  
    * 遍历所有效果完毕   
  * 已经对目标执行完命中处理  --> 判定   
* 技能遍历目标发起命中结束    
* 下一段命中  
* 技能即将结束  --> 判定  
* 技能已经结束   -->判定   

通常1V1战斗一次可能需要保证时序的同步点大约就有20个 而混战情况下则会有N倍的提升,  在非分布式的情况下, 所有的同步点带来的处理复杂度都是一次分支判定,  但是如果是分布式则会是一次rpc   .   
如果是共享内存下的消息队列实现  一次rpc来回则有10ms的延迟  

### 技术选型  
有成功案例的两种做法:   

一种不常见的BIGWORLD的做法(冗余系统&故障切换):   
* 以战斗单位进行解耦, 不同的战斗单位可以分布在不同节点   
* 游戏世界不按照场景地图划分, 而是按照战斗单位的负载动态切分   
* 所有单位进行跨物理节点的冗余, 故障后直接切换到备份单位继续战斗   
  
* 跨节点的战斗, 如果战斗系统同步点过多则不可避免的带来额外的延迟  
* RPC需求让系统变得更为复杂  开发和调试都会带来更多困难    
* 动态负载均衡难以实现  
* 需要面临的技术挑战过大参考资料和技术储备太少   


另外一种, 基于共享内存RESUME做法:   
* 状态数据持久化在共享内存中, 进程crash之后数据不丢失   
* 对使用者透明,  状态数据是在本地内存还是共享内存 对C/C++这种语言的使用者来说没有区别  
  * 经过完善的合理的包装设计, 可以做到业务人员对'共享内存'无感, 基本上做好状态和逻辑分离即可.   
* 对共享内存上的状态访问读写操作等同本地内存, 无额外性能消耗和处理延迟  
* RESUME后保持业务的连贯性, 对用户体验非常友好   
* 原理简单容易(分阶段)实现, 且每阶段都可验证,  有较多成功案例.   



可行性决策:   
* 提高可靠性   
  * 多点备份  故障转移     
    * 可在更多情景下做到可用性 例如网络故障 宕机   
    * 可以考虑在小项目或者中台部门进行MVP迭代到一定完成度   
  * 快速RESUME   
    * 只支持crash情况, 但是根据行业经验 绝大部分情况都是代码bug带来的crash   
* 保障业务连续性  
  * 都能做到业务连续性   
* 成本   
  * 共享内存RESUME方案无论是在开发阶段还是QA/运维部署等阶段成本都大大低于多点备份+故障转移的做法   
* 团队项目   
  * 立项之初团队规模很小 人力资源总预算有限   


### 架构拆分   

* 尽可能的拆分外围服务 以stateless集群+数据库存储方案来实现   
  * 例如好友 聊天 邮件等   

* 对无法做到stateless又难以拆分的管理节点和战斗节点进行RESUME设计.   
  * World管理节点  Scene战斗场景  


## 解决方案   

### 第一个问题  状态和逻辑分离     
这种分离并不一定是要用ECS框架, 只要我们在写代码时候明确的知道在写什么, 但是从规范角度来说 如果一开始采用近似ECS的框架, 那么可能更有助于新人对RESUME框架的认识    

* 业务恢复级别  
  * RESUME机制并不做指令级的RESUME, 例如当前程序call一个函数执行到某处crash 并不会在crash处直接恢复   
    * 如果考虑这种级别的恢复 我们应该做的是一个语言级别的VM  
  * 业务恢复时候会丢弃导致crash的调用栈, 从最初的逻辑循环处重新开始新的TICK.    
    * 受crash影响的调用堆栈中可能会导致相关逻辑出现一定程度的破坏, 因此在写代码时候要做好状态的兜底/超时处理       
    * 例如  
      * 客户端在从地图A进入地图B的过程中会进行多次状态变更以保证能安全的进入地图B, 但是过程中一旦宕机导致中间的一个过程消息包的处理丢失(未完成),  那么应该有TICK定时检测这种状态, 超出正常的处理事件后自动清理流程, 以恢复玩家状态(防止状态被永远改变), 这个服务器编写代码的基本意识, RESUME同样也强调这一点.      
      * 假设策划想同时对某个单位上两个BUFF, 其中一个是功能BUFF, 另外一个事件开关BUFF用来开关功能,  策划要的效果可能是设置一个永久功能BUFF 直到某事件发生则移除功能BUFF,  但是从防御性角度来讲, 一旦出现意外例如条件配置错误 互斥逻辑未考虑充分, 控制BUFF因为resume丢失,  就会出现永久BUFF,  那么实践中我们通常会让策划配置成 开关BUFF定时刷新功能buff  条件不符合时不再刷新,  典型的光环实现, 如果必须要永久 则要有其他的兜底保护 .   这种防御性思想在服务端编程中也是一种基础要求,  RESUME机制也期望这种要求.    

* 聚合业务状态 隔离程序相关的栈状态和全局状态   
  * 全局状态启服会被刷新 栈状态会丢失   



这个更多的是规范和意识问题  项目之初这个情况就比较好 

一些处理中的问题通过, 一旦执行则标记处理,  crash后执行跳过当前正在执行的任务,  通过update和timeout修复导致crash的流程    

### 第二个问题  最小实现验证 实现 first global state    
* 基础的shmget/shmat流程   
  * 启服创建共享内存, 并把global state指针指向共享内存完成构造初始化  
  * 启服绑定共享内存, 并把global state指针指向共享内存完成绑定和resume回调等  

* 所有游戏从global state这个类中聚合   例如  
  * global state: scene server  
    * map<场景>   
      * 场景:   
        * 地图大小   
        * 怪物列表   
    * map<玩家>   
      * 玩家:
        * 技能模块  
          * 技能
          * buff
          * 标签
        * 移动模块  
    * 事件队列   

### 建立一部分规范 确定共享内存的选址等    

#### 技术难点   
* 内存管理   
* 代码段因代码变更或者ASLR随机化   
  * 函数指针变化 虚函数位置变化
* 数据段因ASLR随机化   
  * 地址指针错误  
  * 虚函数表位置变化  
* 数据段因代码变更发生新的定义导致错误  
* 共享内存选址位置RESUME后和stack/heap等冲突   
* STL常用容器无法跑在共享内存下 并且是基础数据结构  
* 队列超时问题  
* 三方库的隔离  

### 实现对象池  实现基础容器    




## 通用性上的挑战      


## 当前效果   



性能对比  






